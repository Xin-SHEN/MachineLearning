{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "breast_cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 30% of the data as labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = breast_cancer.data[:,0:3], breast_cancer.target[:]\n",
    "\n",
    "num_samples = len(labels)\n",
    "shuffle_order = np.random.permutation(num_samples)\n",
    "\n",
    "data = data[shuffle_order, :]\n",
    "labels = labels[shuffle_order]\n",
    "\n",
    "data = data / np.amax(data, axis = 0)\n",
    "labels = labels / np.amax(labels, axis = 0)\n",
    "data = data.astype('float32')\n",
    "labels = labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 3)\n",
      "[[0.4429029  0.3996945  0.43803713]\n",
      " [0.5151192  0.546334   0.5       ]\n",
      " [0.4571327  0.54404277 0.43835545]\n",
      " [0.5969406  0.47861508 0.57984084]\n",
      " [0.7342583  0.4417006  0.71511936]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(data[0:5,:])\n",
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data, labels\n",
    "\n",
    "train_split = 0.6\n",
    "n_train = int(train_split * num_samples)\n",
    "x_train, y_train = X[:n_train], y[:n_train]\n",
    "x_test, y_test = X[n_train:], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, activation='sigmoid', input_dim=3))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 341 samples, validate on 228 samples\n",
      "Epoch 1/200\n",
      "341/341 [==============================] - 0s 688us/step - loss: 0.2789 - val_loss: 0.2885\n",
      "Epoch 2/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.2495 - val_loss: 0.2762\n",
      "Epoch 3/200\n",
      "341/341 [==============================] - 0s 325us/step - loss: 0.2465 - val_loss: 0.2647\n",
      "Epoch 4/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.2458 - val_loss: 0.2755\n",
      "Epoch 5/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.2435 - val_loss: 0.2835\n",
      "Epoch 6/200\n",
      "341/341 [==============================] - 0s 321us/step - loss: 0.2414 - val_loss: 0.2582\n",
      "Epoch 7/200\n",
      "341/341 [==============================] - 0s 328us/step - loss: 0.2370 - val_loss: 0.3027\n",
      "Epoch 8/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.2397 - val_loss: 0.2548\n",
      "Epoch 9/200\n",
      "341/341 [==============================] - 0s 304us/step - loss: 0.2374 - val_loss: 0.2710\n",
      "Epoch 10/200\n",
      "341/341 [==============================] - 0s 319us/step - loss: 0.2360 - val_loss: 0.2574\n",
      "Epoch 11/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2337 - val_loss: 0.2494\n",
      "Epoch 12/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2339 - val_loss: 0.2492\n",
      "Epoch 13/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.2332 - val_loss: 0.2546\n",
      "Epoch 14/200\n",
      "341/341 [==============================] - 0s 297us/step - loss: 0.2293 - val_loss: 0.2467\n",
      "Epoch 15/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.2286 - val_loss: 0.2473\n",
      "Epoch 16/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.2264 - val_loss: 0.2459\n",
      "Epoch 17/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.2231 - val_loss: 0.2424\n",
      "Epoch 18/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2237 - val_loss: 0.2412\n",
      "Epoch 19/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.2212 - val_loss: 0.2647\n",
      "Epoch 20/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2219 - val_loss: 0.2517\n",
      "Epoch 21/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2209 - val_loss: 0.2349\n",
      "Epoch 22/200\n",
      "341/341 [==============================] - 0s 270us/step - loss: 0.2184 - val_loss: 0.2319\n",
      "Epoch 23/200\n",
      "341/341 [==============================] - 0s 291us/step - loss: 0.2165 - val_loss: 0.2296\n",
      "Epoch 24/200\n",
      "341/341 [==============================] - 0s 280us/step - loss: 0.2155 - val_loss: 0.2407\n",
      "Epoch 25/200\n",
      "341/341 [==============================] - 0s 311us/step - loss: 0.2137 - val_loss: 0.2381\n",
      "Epoch 26/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.2136 - val_loss: 0.2296\n",
      "Epoch 27/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2085 - val_loss: 0.2281\n",
      "Epoch 28/200\n",
      "341/341 [==============================] - 0s 301us/step - loss: 0.2088 - val_loss: 0.2370\n",
      "Epoch 29/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.2061 - val_loss: 0.2194\n",
      "Epoch 30/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.2064 - val_loss: 0.2231\n",
      "Epoch 31/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.2042 - val_loss: 0.2258\n",
      "Epoch 32/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.2026 - val_loss: 0.2180\n",
      "Epoch 33/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.2017 - val_loss: 0.2179\n",
      "Epoch 34/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.2005 - val_loss: 0.2163\n",
      "Epoch 35/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1983 - val_loss: 0.2152\n",
      "Epoch 36/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1945 - val_loss: 0.2127\n",
      "Epoch 37/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1945 - val_loss: 0.2095\n",
      "Epoch 38/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1926 - val_loss: 0.2136\n",
      "Epoch 39/200\n",
      "341/341 [==============================] - 0s 285us/step - loss: 0.1922 - val_loss: 0.2019\n",
      "Epoch 40/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1911 - val_loss: 0.2063\n",
      "Epoch 41/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1866 - val_loss: 0.2052\n",
      "Epoch 42/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.1861 - val_loss: 0.1960\n",
      "Epoch 43/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1849 - val_loss: 0.2028\n",
      "Epoch 44/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.1831 - val_loss: 0.1937\n",
      "Epoch 45/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.1815 - val_loss: 0.1965\n",
      "Epoch 46/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.1803 - val_loss: 0.1913\n",
      "Epoch 47/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.1778 - val_loss: 0.1933\n",
      "Epoch 48/200\n",
      "341/341 [==============================] - 0s 325us/step - loss: 0.1760 - val_loss: 0.1845\n",
      "Epoch 49/200\n",
      "341/341 [==============================] - 0s 304us/step - loss: 0.1752 - val_loss: 0.1830\n",
      "Epoch 50/200\n",
      "341/341 [==============================] - 0s 329us/step - loss: 0.1719 - val_loss: 0.1839\n",
      "Epoch 51/200\n",
      "341/341 [==============================] - 0s 297us/step - loss: 0.1705 - val_loss: 0.1853\n",
      "Epoch 52/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1697 - val_loss: 0.1770\n",
      "Epoch 53/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1666 - val_loss: 0.1802\n",
      "Epoch 54/200\n",
      "341/341 [==============================] - 0s 279us/step - loss: 0.1635 - val_loss: 0.1729\n",
      "Epoch 55/200\n",
      "341/341 [==============================] - 0s 316us/step - loss: 0.1635 - val_loss: 0.1741\n",
      "Epoch 56/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1615 - val_loss: 0.1717\n",
      "Epoch 57/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1591 - val_loss: 0.1847\n",
      "Epoch 58/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1600 - val_loss: 0.1757\n",
      "Epoch 59/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1554 - val_loss: 0.1666\n",
      "Epoch 60/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1549 - val_loss: 0.1675\n",
      "Epoch 61/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.1515 - val_loss: 0.1604\n",
      "Epoch 62/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1525 - val_loss: 0.1620\n",
      "Epoch 63/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1494 - val_loss: 0.1567\n",
      "Epoch 64/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.1473 - val_loss: 0.1546\n",
      "Epoch 65/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1473 - val_loss: 0.1565\n",
      "Epoch 66/200\n",
      "341/341 [==============================] - 0s 301us/step - loss: 0.1451 - val_loss: 0.1545\n",
      "Epoch 67/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1434 - val_loss: 0.1499\n",
      "Epoch 68/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.1407 - val_loss: 0.1482\n",
      "Epoch 69/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1407 - val_loss: 0.1514\n",
      "Epoch 70/200\n",
      "341/341 [==============================] - 0s 330us/step - loss: 0.1392 - val_loss: 0.1450\n",
      "Epoch 71/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.1373 - val_loss: 0.1431\n",
      "Epoch 72/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1365 - val_loss: 0.1411\n",
      "Epoch 73/200\n",
      "341/341 [==============================] - 0s 267us/step - loss: 0.1340 - val_loss: 0.1400\n",
      "Epoch 74/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1336 - val_loss: 0.1380\n",
      "Epoch 75/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1321 - val_loss: 0.1364\n",
      "Epoch 76/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1298 - val_loss: 0.1363\n",
      "Epoch 77/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1284 - val_loss: 0.1387\n",
      "Epoch 78/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.1282 - val_loss: 0.1338\n",
      "Epoch 79/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1253 - val_loss: 0.1415\n",
      "Epoch 80/200\n",
      "341/341 [==============================] - 0s 276us/step - loss: 0.1252 - val_loss: 0.1335\n",
      "Epoch 81/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.1239 - val_loss: 0.1325\n",
      "Epoch 82/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1224 - val_loss: 0.1321\n",
      "Epoch 83/200\n",
      "341/341 [==============================] - 0s 276us/step - loss: 0.1218 - val_loss: 0.1260\n",
      "Epoch 84/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1205 - val_loss: 0.1253\n",
      "Epoch 85/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1188 - val_loss: 0.1244\n",
      "Epoch 86/200\n",
      "341/341 [==============================] - 0s 317us/step - loss: 0.1185 - val_loss: 0.1228\n",
      "Epoch 87/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1170 - val_loss: 0.1254\n",
      "Epoch 88/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1168 - val_loss: 0.1208\n",
      "Epoch 89/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1160 - val_loss: 0.1200\n",
      "Epoch 90/200\n",
      "341/341 [==============================] - 0s 261us/step - loss: 0.1146 - val_loss: 0.1186\n",
      "Epoch 91/200\n",
      "341/341 [==============================] - 0s 276us/step - loss: 0.1123 - val_loss: 0.1222\n",
      "Epoch 92/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.1132 - val_loss: 0.1179\n",
      "Epoch 93/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1126 - val_loss: 0.1179\n",
      "Epoch 94/200\n",
      "341/341 [==============================] - 0s 319us/step - loss: 0.1116 - val_loss: 0.1152\n",
      "Epoch 95/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1106 - val_loss: 0.1143\n",
      "Epoch 96/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.1098 - val_loss: 0.1136\n",
      "Epoch 97/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1089 - val_loss: 0.1132\n",
      "Epoch 98/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.1080 - val_loss: 0.1134\n",
      "Epoch 99/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1080 - val_loss: 0.1120\n",
      "Epoch 100/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.1076 - val_loss: 0.1111\n",
      "Epoch 101/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1068 - val_loss: 0.1121\n",
      "Epoch 102/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1054 - val_loss: 0.1098\n",
      "Epoch 103/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1049 - val_loss: 0.1125\n",
      "Epoch 104/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.1048 - val_loss: 0.1122\n",
      "Epoch 105/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.1051 - val_loss: 0.1087\n",
      "Epoch 106/200\n",
      "341/341 [==============================] - 0s 311us/step - loss: 0.1038 - val_loss: 0.1078\n",
      "Epoch 107/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1032 - val_loss: 0.1075\n",
      "Epoch 108/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.0999 - val_loss: 0.1094\n",
      "Epoch 109/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.1034 - val_loss: 0.1074\n",
      "Epoch 110/200\n",
      "341/341 [==============================] - 0s 311us/step - loss: 0.1025 - val_loss: 0.1062\n",
      "Epoch 111/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.1010 - val_loss: 0.1064\n",
      "Epoch 112/200\n",
      "341/341 [==============================] - 0s 311us/step - loss: 0.1021 - val_loss: 0.1055\n",
      "Epoch 113/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.0994 - val_loss: 0.1090\n",
      "Epoch 114/200\n",
      "341/341 [==============================] - 0s 331us/step - loss: 0.1009 - val_loss: 0.1049\n",
      "Epoch 115/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1006 - val_loss: 0.1081\n",
      "Epoch 116/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.1001 - val_loss: 0.1103\n",
      "Epoch 117/200\n",
      "341/341 [==============================] - 0s 316us/step - loss: 0.0992 - val_loss: 0.1045\n",
      "Epoch 118/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.1000 - val_loss: 0.1040\n",
      "Epoch 119/200\n",
      "341/341 [==============================] - 0s 293us/step - loss: 0.0990 - val_loss: 0.1043\n",
      "Epoch 120/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.0986 - val_loss: 0.1040\n",
      "Epoch 121/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.0992 - val_loss: 0.1052\n",
      "Epoch 122/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.0991 - val_loss: 0.1076\n",
      "Epoch 123/200\n",
      "341/341 [==============================] - 0s 325us/step - loss: 0.0986 - val_loss: 0.1031\n",
      "Epoch 124/200\n",
      "341/341 [==============================] - 0s 302us/step - loss: 0.0982 - val_loss: 0.1042\n",
      "Epoch 125/200\n",
      "341/341 [==============================] - 0s 317us/step - loss: 0.0988 - val_loss: 0.1032\n",
      "Epoch 126/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.0979 - val_loss: 0.1053\n",
      "Epoch 127/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0980 - val_loss: 0.1040\n",
      "Epoch 128/200\n",
      "341/341 [==============================] - 0s 267us/step - loss: 0.0981 - val_loss: 0.1037\n",
      "Epoch 129/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.0977 - val_loss: 0.1080\n",
      "Epoch 130/200\n",
      "341/341 [==============================] - 0s 270us/step - loss: 0.0979 - val_loss: 0.1029\n",
      "Epoch 131/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0978 - val_loss: 0.1020\n",
      "Epoch 132/200\n",
      "341/341 [==============================] - 0s 255us/step - loss: 0.0977 - val_loss: 0.1018\n",
      "Epoch 133/200\n",
      "341/341 [==============================] - 0s 267us/step - loss: 0.0966 - val_loss: 0.1073\n",
      "Epoch 134/200\n",
      "341/341 [==============================] - 0s 264us/step - loss: 0.0975 - val_loss: 0.1024\n",
      "Epoch 135/200\n",
      "341/341 [==============================] - 0s 276us/step - loss: 0.0969 - val_loss: 0.1117\n",
      "Epoch 136/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0983 - val_loss: 0.1120\n",
      "Epoch 137/200\n",
      "341/341 [==============================] - 0s 328us/step - loss: 0.0976 - val_loss: 0.1014\n",
      "Epoch 138/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.0974 - val_loss: 0.1016\n",
      "Epoch 139/200\n",
      "341/341 [==============================] - 0s 295us/step - loss: 0.0967 - val_loss: 0.1016\n",
      "Epoch 140/200\n",
      "341/341 [==============================] - 0s 328us/step - loss: 0.0971 - val_loss: 0.1020\n",
      "Epoch 141/200\n",
      "341/341 [==============================] - 0s 322us/step - loss: 0.0966 - val_loss: 0.1051\n",
      "Epoch 142/200\n",
      "341/341 [==============================] - 0s 331us/step - loss: 0.0963 - val_loss: 0.1012\n",
      "Epoch 143/200\n",
      "341/341 [==============================] - 0s 343us/step - loss: 0.0967 - val_loss: 0.1009\n",
      "Epoch 144/200\n",
      "341/341 [==============================] - 0s 328us/step - loss: 0.0966 - val_loss: 0.1009\n",
      "Epoch 145/200\n",
      "341/341 [==============================] - 0s 334us/step - loss: 0.0972 - val_loss: 0.1056\n",
      "Epoch 146/200\n",
      "341/341 [==============================] - 0s 331us/step - loss: 0.0958 - val_loss: 0.1009\n",
      "Epoch 147/200\n",
      "341/341 [==============================] - 0s 319us/step - loss: 0.0962 - val_loss: 0.1008\n",
      "Epoch 148/200\n",
      "341/341 [==============================] - 0s 311us/step - loss: 0.0958 - val_loss: 0.1010\n",
      "Epoch 149/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0968 - val_loss: 0.1007\n",
      "Epoch 150/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.0965 - val_loss: 0.1007\n",
      "Epoch 151/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.0956 - val_loss: 0.1007\n",
      "Epoch 152/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0954 - val_loss: 0.1020\n",
      "Epoch 153/200\n",
      "341/341 [==============================] - 0s 267us/step - loss: 0.0964 - val_loss: 0.1027\n",
      "Epoch 154/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0963 - val_loss: 0.1016\n",
      "Epoch 155/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0957 - val_loss: 0.1009\n",
      "Epoch 156/200\n",
      "341/341 [==============================] - 0s 270us/step - loss: 0.0966 - val_loss: 0.1116\n",
      "Epoch 157/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0964 - val_loss: 0.1006\n",
      "Epoch 158/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.0958 - val_loss: 0.1028\n",
      "Epoch 159/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0960 - val_loss: 0.1007\n",
      "Epoch 160/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.0958 - val_loss: 0.1074\n",
      "Epoch 161/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.0957 - val_loss: 0.1031\n",
      "Epoch 162/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.0956 - val_loss: 0.1032\n",
      "Epoch 163/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.0959 - val_loss: 0.1003\n",
      "Epoch 164/200\n",
      "341/341 [==============================] - 0s 317us/step - loss: 0.0954 - val_loss: 0.1012\n",
      "Epoch 165/200\n",
      "341/341 [==============================] - 0s 273us/step - loss: 0.0959 - val_loss: 0.1030\n",
      "Epoch 166/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.0949 - val_loss: 0.1004\n",
      "Epoch 167/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.0961 - val_loss: 0.1016\n",
      "Epoch 168/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.0955 - val_loss: 0.1010\n",
      "Epoch 169/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0952 - val_loss: 0.1013\n",
      "Epoch 170/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.0956 - val_loss: 0.1004\n",
      "Epoch 171/200\n",
      "341/341 [==============================] - 0s 314us/step - loss: 0.0944 - val_loss: 0.1021\n",
      "Epoch 172/200\n",
      "341/341 [==============================] - 0s 319us/step - loss: 0.0951 - val_loss: 0.1029\n",
      "Epoch 173/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.0956 - val_loss: 0.1002\n",
      "Epoch 174/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0959 - val_loss: 0.1010\n",
      "Epoch 175/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.0961 - val_loss: 0.1001\n",
      "Epoch 176/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.0958 - val_loss: 0.1007\n",
      "Epoch 177/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0956 - val_loss: 0.1144\n",
      "Epoch 178/200\n",
      "341/341 [==============================] - 0s 287us/step - loss: 0.0932 - val_loss: 0.1015\n",
      "Epoch 179/200\n",
      "341/341 [==============================] - 0s 317us/step - loss: 0.0960 - val_loss: 0.1017\n",
      "Epoch 180/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0956 - val_loss: 0.1043\n",
      "Epoch 181/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0960 - val_loss: 0.1001\n",
      "Epoch 182/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0955 - val_loss: 0.1099\n",
      "Epoch 183/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.0962 - val_loss: 0.1026\n",
      "Epoch 184/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.0957 - val_loss: 0.1023\n",
      "Epoch 185/200\n",
      "341/341 [==============================] - 0s 328us/step - loss: 0.0958 - val_loss: 0.1005\n",
      "Epoch 186/200\n",
      "341/341 [==============================] - 0s 299us/step - loss: 0.0947 - val_loss: 0.1115\n",
      "Epoch 187/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0958 - val_loss: 0.1010\n",
      "Epoch 188/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0946 - val_loss: 0.1027\n",
      "Epoch 189/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.0955 - val_loss: 0.1008\n",
      "Epoch 190/200\n",
      "341/341 [==============================] - 0s 305us/step - loss: 0.0956 - val_loss: 0.1002\n",
      "Epoch 191/200\n",
      "341/341 [==============================] - 0s 308us/step - loss: 0.0958 - val_loss: 0.1030\n",
      "Epoch 192/200\n",
      "341/341 [==============================] - 0s 290us/step - loss: 0.0953 - val_loss: 0.1056\n",
      "Epoch 193/200\n",
      "341/341 [==============================] - 0s 296us/step - loss: 0.0952 - val_loss: 0.1106\n",
      "Epoch 194/200\n",
      "341/341 [==============================] - 0s 275us/step - loss: 0.0949 - val_loss: 0.1001\n",
      "Epoch 195/200\n",
      "341/341 [==============================] - 0s 278us/step - loss: 0.0961 - val_loss: 0.1054\n",
      "Epoch 196/200\n",
      "341/341 [==============================] - 0s 281us/step - loss: 0.0951 - val_loss: 0.1005\n",
      "Epoch 197/200\n",
      "341/341 [==============================] - 0s 284us/step - loss: 0.0958 - val_loss: 0.1008\n",
      "Epoch 198/200\n",
      "341/341 [==============================] - 0s 261us/step - loss: 0.0965 - val_loss: 0.1018\n",
      "Epoch 199/200\n",
      "341/341 [==============================] - 0s 261us/step - loss: 0.0959 - val_loss: 0.1100\n",
      "Epoch 200/200\n",
      "341/341 [==============================] - 0s 343us/step - loss: 0.0960 - val_loss: 0.1014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1329ca14d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,\n",
    "         batch_size=4, epochs=200,\n",
    "         verbose=1,\n",
    "         validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 0s 31us/step\n",
      "0.10144685862357156\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted 0.664, actual 1.000\n",
      "predicted 0.504, actual 0.000\n",
      "predicted 0.025, actual 0.000\n",
      "predicted 0.953, actual 1.000\n",
      "predicted 0.628, actual 1.000\n",
      "predicted 0.912, actual 1.000\n",
      "predicted 0.048, actual 0.000\n",
      "predicted -0.190, actual 0.000\n",
      "predicted 0.533, actual 0.000\n",
      "predicted 0.687, actual 1.000\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "for yp,ya in list(zip(y_pred, y_test))[0:10]:\n",
    "    print('predicted %0.3f, actual %0.3f' % (yp, ya))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accuracy : 0.877\n"
     ]
    }
   ],
   "source": [
    "corrent_count = 0\n",
    "for yp,ya in list(zip(y_pred,y_test))[:]:\n",
    "    if(np.around(yp)==ya):\n",
    "        corrent_count +=1\n",
    "\n",
    "print('predict accuracy : %0.3f'%(corrent_count/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network with more neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(16, activation='sigmoid', input_dim=3))\n",
    "model2.add(Dense(1, activation='linear'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 0s 53us/step\n",
      "0.10177660458966305\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model2.fit(x_train,y_train,\n",
    "         batch_size=4, epochs=200,\n",
    "         verbose=0,\n",
    "         validation_data=(x_test,y_test))\n",
    "score2 = model2.evaluate(x_test, y_test)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accuracy2 : 0.877\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model2.predict(x_test)\n",
    "\n",
    "corrent_count2 = 0\n",
    "for yp,ya in list(zip(y_pred2,y_test))[:]:\n",
    "    if(np.around(yp)==ya):\n",
    "        corrent_count2 +=1\n",
    "\n",
    "print('predict accuracy2 : %0.3f'%(corrent_count2/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network with more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 113\n",
      "Trainable params: 113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(8, activation='sigmoid', input_dim=3))\n",
    "model3.add(Dense(8, activation='sigmoid'))\n",
    "model3.add(Dense(1, activation='linear'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 0s 35us/step\n",
      "0.11959696312745412\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model3.fit(x_train,y_train,\n",
    "         batch_size=4, epochs=200,\n",
    "         verbose=0,\n",
    "         validation_data=(x_test,y_test))\n",
    "score3 = model3.evaluate(x_test, y_test)\n",
    "print(score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accuracy3 : 0.864\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = model3.predict(x_test)\n",
    "\n",
    "corrent_count3 = 0\n",
    "for yp,ya in list(zip(y_pred3,y_test))[:]:\n",
    "    if(np.around(yp)==ya):\n",
    "        corrent_count3 +=1\n",
    "\n",
    "print('predict accuracy3 : %0.3f'%(corrent_count3/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network with more data, more layers, more neurons and more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "[[0.6118819  0.6242362  0.60583556 0.37161136 0.65544677 0.5298205\n",
      "  0.3964386  0.394831   0.63388157 0.665743   0.2056039  0.21310133\n",
      "  0.16856232 0.12812616 0.18695793 0.41477105 0.10737374 0.2134874\n",
      "  0.19341356 0.21109249 0.64705884 0.68268067 0.60350317 0.3951575\n",
      "  0.71203953 0.6988658  0.5244409  0.65257734 0.49909613 0.6453012 ]\n",
      " [0.45464247 0.4198065  0.43167108 0.20091963 0.6016524  0.15153445\n",
      "  0.08559044 0.14234592 0.5230263  0.5801519  0.08242255 0.17875127\n",
      "  0.06692448 0.03380672 0.25576615 0.04144756 0.04002525 0.1640841\n",
      "  0.28549716 0.06387399 0.3734739  0.3988696  0.341043   0.13044193\n",
      "  0.58221024 0.06673913 0.08298722 0.20213059 0.35899368 0.30891567]\n",
      " [0.5467805  0.57942975 0.531565   0.29116353 0.5630355  0.2999421\n",
      "  0.26288658 0.3719185  0.56480265 0.6257184  0.10891055 0.17222108\n",
      "  0.094404   0.05429731 0.31744298 0.18050222 0.11441919 0.33396477\n",
      "  0.3129829  0.07178284 0.45588234 0.5215987  0.42794585 0.19532205\n",
      "  0.56469005 0.18875237 0.22731629 0.5072165  0.38505575 0.32906023]\n",
      " [0.36499467 0.3110998  0.34880635 0.12858856 0.6117503  0.21835554\n",
      "  0.04505623 0.09781312 0.59210527 0.67415845 0.06651584 0.11211873\n",
      "  0.06132848 0.02191073 0.18252489 0.10081241 0.02145454 0.13125592\n",
      "  0.24547182 0.0794571  0.31576025 0.31590635 0.2915207  0.09273625\n",
      "  0.60332435 0.15595463 0.0688099  0.23010309 0.44245255 0.37214458]\n",
      " [0.39630026 0.35819757 0.37793103 0.1537785  0.44516525 0.17556456\n",
      "  0.10555295 0.07311133 0.5559211  0.6242816  0.1469544  0.16564995\n",
      "  0.15150136 0.0531907  0.1779955  0.2501477  0.11376262 0.27865127\n",
      "  0.3929069  0.16189678 0.336293   0.3193379  0.3169586  0.10660554\n",
      "  0.39820305 0.11871456 0.09592652 0.13477664 0.3880687  0.33821687]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = breast_cancer.data[:,:], breast_cancer.target[:]\n",
    "\n",
    "num_samples = len(labels)\n",
    "shuffle_order = np.random.permutation(num_samples)\n",
    "\n",
    "data = data[shuffle_order, :]\n",
    "labels = labels[shuffle_order]\n",
    "\n",
    "data = data / np.amax(data, axis = 0)\n",
    "labels = labels / np.amax(labels, axis = 0)\n",
    "data = data.astype('float32')\n",
    "labels = labels.astype('float32')\n",
    "\n",
    "print(data.shape)\n",
    "print(data[0:5,:])\n",
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data, labels\n",
    "\n",
    "train_split = 0.6\n",
    "n_train = int(train_split * num_samples)\n",
    "x_train, y_train = X[:n_train], y[:n_train]\n",
    "x_test, y_test = X[n_train:], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 785\n",
      "Trainable params: 785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(16, activation='sigmoid', input_dim=30))\n",
    "model4.add(Dense(16, activation='sigmoid'))\n",
    "model4.add(Dense(1, activation='linear'))\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228 [==============================] - 0s 44us/step\n",
      "0.060652812060556914\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss='mean_squared_error',optimizer='sgd')\n",
    "model4.fit(x_train,y_train,\n",
    "         batch_size=4, epochs=1000,\n",
    "         verbose=0,\n",
    "         validation_data=(x_test,y_test))\n",
    "score4 = model4.evaluate(x_test, y_test)\n",
    "print(score4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict accuracy4 : 0.952\n"
     ]
    }
   ],
   "source": [
    "y_pred4 = model4.predict(x_test)\n",
    "\n",
    "corrent_count4 = 0\n",
    "for yp,ya in list(zip(y_pred4,y_test))[:]:\n",
    "    if(np.around(yp)==ya):\n",
    "        corrent_count4 +=1\n",
    "\n",
    "print('predict accuracy4 : %0.3f'%(corrent_count4/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Xin_wk4_Model')\n",
    "#keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xin's question\n",
    "Keras' offical doc indicate that with TensorFlow backend, my code will automatically run on GPU if any available GPU is detected. \n",
    "However my Quadro-K4200 loaded nothing during the training task.\n",
    "\n",
    "How can I enable my GPU function?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
